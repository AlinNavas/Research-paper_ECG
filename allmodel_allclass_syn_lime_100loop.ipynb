{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 16:01:39.383679: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-21 16:01:39.403344: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-21 16:01:39.403367: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-21 16:01:39.403867: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-21 16:01:39.407367: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-21 16:01:39.836833: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Imports:\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import *\n",
    "import os\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import *\n",
    "sns.set()\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "length = 277\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6466060840889766009\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5803868160\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10671326534856521941\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 16:01:41.922286: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-21 16:01:41.943515: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-21 16:01:41.943547: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-21 16:01:42.183326: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-21 16:01:42.183362: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-21 16:01:42.183366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-03-21 16:01:42.183388: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-21 16:01:42.183400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 5535 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38751, 277)\n",
      "(72794, 277)\n"
     ]
    }
   ],
   "source": [
    "train_values = pd.read_csv('./data/train_test/train_patients.csv').values\n",
    "test_values = pd.read_csv('./data/train_test/test_patients.csv').values\n",
    "    \n",
    "print(train_values.shape)\n",
    "print(test_values.shape)\n",
    "\n",
    "# Separate the training and testing data, and one-hot encode Y:\n",
    "X_train = train_values[:,:-2]\n",
    "X_test = test_values[:,:-2]\n",
    "\n",
    "X_train = X_train.reshape(-1, X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(-1, X_train.shape[1], 1)\n",
    "\n",
    "y_train = train_values[:,-2]\n",
    "y_test = test_values[:,-2]\n",
    "\n",
    "y_train=to_categorical(y_train)\n",
    "y_test=to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=2.0):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt = y_pred * y_true + (1 - y_pred) * (1 - y_true)\n",
    "        loss = -K.pow(1 - pt, gamma) * K.log(pt)\n",
    "        return loss\n",
    "\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "model_files = ['./models/mlpmodel.h5', './models/cnnmodel.h5', './models/lstmmodel.h5']\n",
    "model_names = ['MLP','CNN', 'LSTM']\n",
    "\n",
    "for model_file, model_name in zip(model_files, model_names):\n",
    "        if model_name == 'LSTM':\n",
    "                model = load_model(model_file, custom_objects={'focal_loss_fixed': focal_loss})\n",
    "        else:\n",
    "                model = load_model(model_file)\n",
    "\n",
    "        feature_importance_df = pd.DataFrame()\n",
    "\n",
    "        for class_num in range(1, 7):\n",
    "\n",
    "            N_test = test_values[test_values[:,-2]==class_num]\n",
    "\n",
    "            Xs_test = N_test[:,:-2]\n",
    "            ys_test = N_test[:len(N_test),-2]\n",
    "\n",
    "            Xs_test1 = Xs_test.reshape(-1, X_test.shape[1], 1)\n",
    "            ys_test1 = to_categorical(ys_test)\n",
    "\n",
    "\n",
    "            ecg_normalized_all = []\n",
    "\n",
    "            for n in range (0, len(Xs_test1)):\n",
    "                ecg_normalized_temp = (Xs_test1[n, :, 0] - Xs_test1[n, :, 0].min()) / (Xs_test1[n, :, 0].max() - Xs_test1[n, :, 0].min())\n",
    "                ecg_normalized_all.append(ecg_normalized_temp)\n",
    "            ecg_normalized_all = np.array(ecg_normalized_all)\n",
    "\n",
    "            ecg_class_mean = np.mean(ecg_normalized_all, axis=0)\n",
    "\n",
    "            # Initialize empty lists to store the weights and features\n",
    "            all_weights = []\n",
    "            all_features = []\n",
    "\n",
    "            # Repeat the code block 100 times\n",
    "            for _ in range(100):\n",
    "                # Your code block here\n",
    "                explainer = lime.lime_tabular.RecurrentTabularExplainer(X_train, feature_names=['x'], discretize_continuous=True)\n",
    "                exp = explainer.explain_instance(ecg_class_mean, model.predict, labels=[class_num], num_features=275)\n",
    "\n",
    "                # Get the weights for all features\n",
    "                map_explanation = exp.as_map()[class_num]\n",
    "                features, weights = [], []\n",
    "                for e in map_explanation:\n",
    "                    features.append(e[0])\n",
    "                    weights.append(e[1])\n",
    "                features = np.array(features)\n",
    "                weights = np.array(weights)\n",
    "\n",
    "                # Append the weights and features to the lists\n",
    "                all_weights.append(weights)\n",
    "                all_features.append(features)\n",
    "\n",
    "            # Calculate the average weights and features\n",
    "            average_weights = np.mean(all_weights, axis=0)\n",
    "            average_features = np.mean(all_features, axis=0)\n",
    "\n",
    "            # Update the average_weights and average_features as the new feature and weights\n",
    "\n",
    "            features = average_features\n",
    "            weights = average_weights\n",
    "\n",
    "            # Sort based on features:\n",
    "            indices_sort = np.argsort(features)\n",
    "            features = features[indices_sort]\n",
    "            weights = weights[indices_sort]\n",
    "\n",
    "            # Average over the actual variables:\n",
    "            mean_weights = np.zeros((23, ))\n",
    "            for i in range(0, 275, 12):\n",
    "                mean_weights[i // 12] = np.mean(weights[i:i+12])\n",
    "\n",
    "            # Sort based on weight:\n",
    "            indices_sort = np.argsort(-1 * mean_weights)\n",
    "            slices = np.arange(1, 24)\n",
    "\n",
    "            # Visualize:\n",
    "            feature_importance_normalized = (mean_weights - mean_weights.min()) / (mean_weights.max() - mean_weights.min())\n",
    "\n",
    "            fig, ax = plt.subplots(1, 2, figsize = (15, 4))\n",
    "            ax[0].bar(range(23), feature_importance_normalized[indices_sort])\n",
    "            ax[0].set_title('LIME weights for Synthetic ECG for class ' + str(class_num) + ' with a '+ str(model_name) + ' model')\n",
    "            ax[0].set_xticks(np.arange(23))\n",
    "            ax[0].set_xticklabels(slices[indices_sort])\n",
    "            ax[0].set_ylabel('LIME weights')\n",
    "            ax[0].set_xlabel('Segment')\n",
    "\n",
    "            ecg_normalized = (ecg_class_mean - ecg_class_mean.min()) / ecg_class_mean.max() - ecg_class_mean.min()\n",
    "\n",
    "            ax[1].plot(np.arange(275), ecg_normalized, label = 'Synthetic ECG for class ' + str(class_num))\n",
    "\n",
    "            ax[1].plot(np.repeat(feature_importance_normalized , 12), label = 'LIME weight')\n",
    "            ax[1].set_title('LIME weights for Synthetic ECG for class ' + str(class_num) + ' with a '+ str(model_name) + ' model')\n",
    "            ax[1].set_ylabel('ECG signal / LIME weights')\n",
    "            ax[1].set_xlabel('Time')\n",
    "            ax[1].legend()\n",
    "            plt.savefig('./output_pics/LIME weights for Synthetic ECG for class ' + str(class_num) + ' with a '+ str(model_name) + ' model', dpi = 400)\n",
    "\n",
    "        # Append the importance_dict to the feature_importance_df DataFrame\n",
    "            feature_importance_df['class_' + str(class_num)] = feature_importance_normalized\n",
    "    # Save the feature_importance_df DataFrame to a CSV file\n",
    "        feature_importance_df.to_csv('./output_csv/syn_lime_'+ str(model_name) + '.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
