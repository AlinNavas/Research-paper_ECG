{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import *\n",
    "import os\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import *\n",
    "from sklearn.metrics import *\n",
    "sns.set()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "length = 277"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5990976684985645884\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5803868160\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 16514700651497956748\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-19 21:37:45.764094: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-19 21:37:45.764170: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-19 21:37:45.764189: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-19 21:37:45.764514: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-19 21:37:45.764530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-03-19 21:37:45.764558: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-19 21:37:45.764581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 5535 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading  ./data/train_test/train_patients.csv\n",
      "Loading  ./data/train_test/test_patients.csv\n",
      "(38752, 277)\n",
      "(72795, 277)\n",
      "X_train shape: (27126, 275)\n",
      "X_val shape: (11626, 275)\n",
      "y_train shape: (27126,)\n",
      "y_val shape: (11626,)\n"
     ]
    }
   ],
   "source": [
    "# Load the training and testing data:\n",
    "train_values = np.empty(shape=[0, length])\n",
    "test_values = np.empty(shape=[0, length])\n",
    "\n",
    "train_beats = glob.glob('./data/train_test/train_patients.csv')\n",
    "test_beats = glob.glob('./data/train_test/test_patients.csv')\n",
    "\n",
    "for j in train_beats:\n",
    "    print('Loading ', j)\n",
    "    csvrows = np.loadtxt(j, delimiter=',')\n",
    "    train_values = np.append(train_values, csvrows, axis=0)\n",
    "\n",
    "for j in test_beats:\n",
    "    print('Loading ', j)\n",
    "    csvrows = np.loadtxt(j, delimiter=',')\n",
    "    test_values = np.append(test_values, csvrows, axis=0)\n",
    "    \n",
    "print(train_values.shape)\n",
    "print(test_values.shape)\n",
    "\n",
    "# Separate the training and testing data, and one-hot encode Y:\n",
    "X_train = train_values[:,:-2]\n",
    "X_test = test_values[:,:-2]\n",
    "y_train = train_values[:,-2]\n",
    "y_test = test_values[:,-2]\n",
    "X_test1 = X_test.reshape(-1, X_train.shape[1], 1)\n",
    "y_test1=to_categorical(y_test)\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42,stratify=y_train)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "X_train1 = X_train.reshape(-1, X_train.shape[1], 1)\n",
    "y_train1=to_categorical(y_train)\n",
    "\n",
    "X_val1 = X_val.reshape(-1, X_train.shape[1], 1)\n",
    "y_val1=to_categorical(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 275, 64)           16896     \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 275, 64)           33024     \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 275, 64)           33024     \n",
      "                                                                 \n",
      " lstm_15 (LSTM)              (None, 275, 64)           33024     \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 17600)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7)                 123207    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 239175 (934.28 KB)\n",
      "Trainable params: 239175 (934.28 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM, Dropout, Dense,Flatten\n",
    "from keras.optimizers import Nadam\n",
    "from keras.losses import categorical_crossentropy  \n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "\n",
    "def focal_loss(gamma=2.0):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt = y_pred * y_true + (1 - y_pred) * (1 - y_true)\n",
    "        loss = -K.pow(1 - pt, gamma) * K.log(pt)\n",
    "        return loss\n",
    "\n",
    "    return focal_loss_fixed\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(64, return_sequences=True, input_shape=(X_train1.shape[1],X_train1.shape[2])))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Flatten())\n",
    "# Add a Dense layer\n",
    "model.add(Dense(7, activation='softmax'))  # Assuming 10 classes for classification\n",
    "\n",
    "# Compile the model with focal loss\n",
    "model.compile(loss=focal_loss(gamma=2.0), optimizer=Nadam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25926138, 2.33402168, 2.33281734, 2.33281734, 2.33402168,\n",
       "       2.33281734])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "848/848 [==============================] - 64s 72ms/step - loss: 0.0269 - accuracy: 0.9045 - val_loss: 0.0161 - val_accuracy: 0.9453\n",
      "Epoch 2/50\n",
      "848/848 [==============================] - 61s 72ms/step - loss: 0.0135 - accuracy: 0.9513 - val_loss: 0.0104 - val_accuracy: 0.9626\n",
      "Epoch 3/50\n",
      "848/848 [==============================] - 63s 74ms/step - loss: 0.0102 - accuracy: 0.9617 - val_loss: 0.0085 - val_accuracy: 0.9692\n",
      "Epoch 4/50\n",
      "848/848 [==============================] - 67s 79ms/step - loss: 0.0087 - accuracy: 0.9674 - val_loss: 0.0079 - val_accuracy: 0.9702\n",
      "Epoch 5/50\n",
      "848/848 [==============================] - 72s 85ms/step - loss: 0.0074 - accuracy: 0.9713 - val_loss: 0.0066 - val_accuracy: 0.9766\n",
      "Epoch 6/50\n",
      "848/848 [==============================] - 73s 86ms/step - loss: 0.0063 - accuracy: 0.9754 - val_loss: 0.0094 - val_accuracy: 0.9658\n",
      "Epoch 7/50\n",
      "848/848 [==============================] - 70s 83ms/step - loss: 0.0056 - accuracy: 0.9772 - val_loss: 0.0059 - val_accuracy: 0.9768\n",
      "Epoch 8/50\n",
      "848/848 [==============================] - 66s 78ms/step - loss: 0.0050 - accuracy: 0.9786 - val_loss: 0.0064 - val_accuracy: 0.9773\n",
      "Epoch 9/50\n",
      "848/848 [==============================] - 74s 87ms/step - loss: 0.0044 - accuracy: 0.9821 - val_loss: 0.0060 - val_accuracy: 0.9794\n",
      "Epoch 10/50\n",
      "848/848 [==============================] - 69s 81ms/step - loss: 0.0038 - accuracy: 0.9843 - val_loss: 0.0059 - val_accuracy: 0.9809\n"
     ]
    }
   ],
   "source": [
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode='min',min_delta=0.001, restore_best_weights=True)\n",
    "history = model.fit(X_train1, y_train1, validation_data=(X_val1, y_val1), epochs=50,callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0 58952  2493  6725  2923  1277   425]\n"
     ]
    }
   ],
   "source": [
    "class_counts = np.bincount(y_test.astype(int))\n",
    "print(class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2275/2275 [==============================] - 69s 30ms/step\n",
      "Confusion Matrix:\n",
      "[[58351     2    14   198   312    75]\n",
      " [    1  2488     0     4     0     0]\n",
      " [   40     0  6626     6    46     7]\n",
      " [   49     0     4  2833    10    27]\n",
      " [  157     0     3     8  1109     0]\n",
      " [   15     0     0     7     1   402]]\n",
      "F1 Score: 0.9868334134929451\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "# Predict the test data\n",
    "y_pred = model.predict(X_test1)\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_test, y_pred_labels, average='weighted')\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/lstmmodel.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
