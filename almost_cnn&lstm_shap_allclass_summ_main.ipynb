{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-22 12:28:03.155563: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-22 12:28:03.179865: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-22 12:28:03.179887: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-22 12:28:03.180444: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-22 12:28:03.184579: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-22 12:28:03.687950: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/alinnavas/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports:\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import *\n",
    "import os\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "sns.set_theme()\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "length = 277\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-22 12:28:06.237588: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-22 12:28:06.296156: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-22 12:28:06.296194: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-22 12:28:06.576463: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-22 12:28:06.576509: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-22 12:28:06.576515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-03-22 12:28:06.576543: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-22 12:28:06.576558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 5535 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4604309519108545424\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5803868160\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 17250167677413551242\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38751, 277)\n",
      "(72794, 277)\n"
     ]
    }
   ],
   "source": [
    "train_values = pd.read_csv('./data/train_test/train_patients.csv').values\n",
    "test_values = pd.read_csv('./data/train_test/test_patients.csv').values\n",
    "\n",
    "print(train_values.shape)\n",
    "print(test_values.shape)\n",
    "\n",
    "X_train = train_values[:,:-2]\n",
    "X_test = test_values[:,:-2]\n",
    "\n",
    "X_train = X_train.reshape(-1, X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(-1, X_test.shape[1], 1)\n",
    "\n",
    "y_train = train_values[:,-2]\n",
    "y_test = test_values[:,-2]\n",
    "\n",
    "y_train=to_categorical(y_train)\n",
    "y_test=to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=2.0):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt = y_pred * y_true + (1 - y_pred) * (1 - y_true)\n",
    "        loss = -K.pow(1 - pt, gamma) * K.log(pt)\n",
    "        return loss\n",
    "\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of each class in test values:\n",
      "{1.0: 58951, 2.0: 2493, 3.0: 6725, 4.0: 2923, 5.0: 1277, 6.0: 425}\n"
     ]
    }
   ],
   "source": [
    "unique_classes, class_counts = np.unique(test_values[:, -2], return_counts=True)\n",
    "class_counts_dict = dict(zip(unique_classes, class_counts))\n",
    "print(\"Count of each class in test values:\")\n",
    "print(class_counts_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-22 12:28:08.590229: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-22 12:28:08.590309: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-22 12:28:08.590328: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-22 12:28:08.590439: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-22 12:28:08.590456: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-22 12:28:08.590471: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-22 12:28:08.590781: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-22 12:28:08.590794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-03-22 12:28:08.590822: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-22 12:28:08.590832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5535 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2024-03-22 12:28:08.999946: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.01GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-22 12:28:08.999987: W tensorflow/core/kernels/gpu_utils.cc:54] Failed to allocate memory for convolution redzone checking; skipping this check. This is benign and only means that we won't check cudnn for out-of-bounds reads and writes. This message will only be printed once.\n",
      "2024-03-22 12:28:09.031710: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-03-22 12:28:09.282725: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-03-22 12:28:10.348614: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.05GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-22 12:28:10.413415: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.05GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-22 12:28:10.413470: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.01GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-22 12:28:10.413482: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.05GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-22 12:28:10.413490: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.01GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-22 12:28:11.277914: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.05GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-22 12:28:11.333959: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.05GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-22 12:28:11.334009: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.01GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-22 12:28:11.334019: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.05GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-22 12:28:21.338569: W external/local_tsl/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.00GiB (rounded to 4292640000)requested by op BiasAdd\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2024-03-22 12:28:21.338670: I external/local_tsl/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc\n",
      "2024-03-22 12:28:21.338686: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (256): \tTotal Chunks: 36, Chunks in use: 36. 9.0KiB allocated for chunks. 9.0KiB in use in bin. 1.2KiB client-requested in use in bin.\n",
      "2024-03-22 12:28:21.338692: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (512): \tTotal Chunks: 7, Chunks in use: 7. 4.8KiB allocated for chunks. 4.8KiB in use in bin. 3.8KiB client-requested in use in bin.\n",
      "2024-03-22 12:28:21.338697: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (1024): \tTotal Chunks: 6, Chunks in use: 6. 8.5KiB allocated for chunks. 8.5KiB in use in bin. 6.7KiB client-requested in use in bin.\n",
      "2024-03-22 12:28:21.338702: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (2048): \tTotal Chunks: 4, Chunks in use: 3. 12.2KiB allocated for chunks. 8.5KiB in use in bin. 7.0KiB client-requested in use in bin.\n",
      "2024-03-22 12:28:21.338708: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (4096): \tTotal Chunks: 3, Chunks in use: 2. 20.0KiB allocated for chunks. 15.5KiB in use in bin. 15.5KiB client-requested in use in bin.\n",
      "2024-03-22 12:28:21.338712: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (8192): \tTotal Chunks: 1, Chunks in use: 1. 15.2KiB allocated for chunks. 15.2KiB in use in bin. 7.7KiB client-requested in use in bin.\n",
      "2024-03-22 12:28:21.338718: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-22 12:28:21.338724: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (32768): \tTotal Chunks: 2, Chunks in use: 2. 72.0KiB allocated for chunks. 72.0KiB in use in bin. 71.7KiB client-requested in use in bin.\n",
      "2024-03-22 12:28:21.338729: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (65536): \tTotal Chunks: 1, Chunks in use: 1. 64.2KiB allocated for chunks. 64.2KiB in use in bin. 35.9KiB client-requested in use in bin.\n",
      "2024-03-22 12:28:21.338735: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (131072): \tTotal Chunks: 3, Chunks in use: 3. 407.0KiB allocated for chunks. 407.0KiB in use in bin. 385.7KiB client-requested in use in bin.\n",
      "2024-03-22 12:28:21.338739: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-22 12:28:21.338744: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (524288): \tTotal Chunks: 3, Chunks in use: 3. 1.94MiB allocated for chunks. 1.94MiB in use in bin. 1.74MiB client-requested in use in bin.\n",
      "2024-03-22 12:28:21.338748: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-22 12:28:21.338752: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): \tTotal Chunks: 4, Chunks in use: 3. 8.41MiB allocated for chunks. 6.38MiB in use in bin. 6.09MiB client-requested in use in bin.\n",
      "2024-03-22 12:28:21.338756: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-22 12:28:21.338761: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): \tTotal Chunks: 1, Chunks in use: 1. 10.49MiB allocated for chunks. 10.49MiB in use in bin. 10.49MiB client-requested in use in bin.\n",
      "2024-03-22 12:28:21.338765: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-22 12:28:21.338769: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-22 12:28:21.338775: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-22 12:28:21.338779: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-22 12:28:21.338784: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): \tTotal Chunks: 2, Chunks in use: 1. 5.38GiB allocated for chunks. 4.00GiB in use in bin. 4.00GiB client-requested in use in bin.\n",
      "2024-03-22 12:28:21.338790: I external/local_tsl/tsl/framework/bfc_allocator.cc:1062] Bin for 4.00GiB was 256.00MiB, Chunk State: \n",
      "2024-03-22 12:28:21.338801: I external/local_tsl/tsl/framework/bfc_allocator.cc:1068]   Size: 1.39GiB | Requested Size: 10.50MiB | in_use: 0 | bin_num: 20, prev:   Size: 4.00GiB | Requested Size: 4.00GiB | in_use: 1 | bin_num: -1\n",
      "2024-03-22 12:28:21.338805: I external/local_tsl/tsl/framework/bfc_allocator.cc:1075] Next region of size 5803868160\n",
      "2024-03-22 12:28:21.338810: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707800000 of size 256 next 1\n",
      "2024-03-22 12:28:21.338814: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707800100 of size 1280 next 2\n",
      "2024-03-22 12:28:21.338817: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707800600 of size 256 next 3\n",
      "2024-03-22 12:28:21.338820: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707800700 of size 256 next 4\n",
      "2024-03-22 12:28:21.338823: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707800800 of size 256 next 6\n",
      "2024-03-22 12:28:21.338827: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707800900 of size 768 next 14\n",
      "2024-03-22 12:28:21.338830: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707800c00 of size 1024 next 7\n",
      "2024-03-22 12:28:21.338833: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707801000 of size 256 next 5\n",
      "2024-03-22 12:28:21.338836: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707801100 of size 256 next 8\n",
      "2024-03-22 12:28:21.338839: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707801200 of size 768 next 13\n",
      "2024-03-22 12:28:21.338842: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707801500 of size 256 next 11\n",
      "2024-03-22 12:28:21.338845: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707801600 of size 256 next 12\n",
      "2024-03-22 12:28:21.338848: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707801700 of size 256 next 34\n",
      "2024-03-22 12:28:21.338851: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707801800 of size 256 next 18\n",
      "2024-03-22 12:28:21.338854: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707801900 of size 256 next 16\n",
      "2024-03-22 12:28:21.338857: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707801a00 of size 256 next 17\n",
      "2024-03-22 12:28:21.338860: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707801b00 of size 256 next 23\n",
      "2024-03-22 12:28:21.338863: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707801c00 of size 256 next 21\n",
      "2024-03-22 12:28:21.338866: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707801d00 of size 256 next 22\n",
      "2024-03-22 12:28:21.338875: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707801e00 of size 256 next 28\n",
      "2024-03-22 12:28:21.338878: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707801f00 of size 256 next 26\n",
      "2024-03-22 12:28:21.338881: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707802000 of size 256 next 27\n",
      "2024-03-22 12:28:21.338885: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707802100 of size 256 next 32\n",
      "2024-03-22 12:28:21.338888: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707802200 of size 256 next 33\n",
      "2024-03-22 12:28:21.338891: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707802300 of size 256 next 31\n",
      "2024-03-22 12:28:21.338894: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707802400 of size 256 next 40\n",
      "2024-03-22 12:28:21.338897: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707802500 of size 256 next 39\n",
      "2024-03-22 12:28:21.338900: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707802600 of size 256 next 41\n",
      "2024-03-22 12:28:21.338903: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707802700 of size 256 next 42\n",
      "2024-03-22 12:28:21.338906: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707802800 of size 256 next 43\n",
      "2024-03-22 12:28:21.338909: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707802900 of size 256 next 44\n",
      "2024-03-22 12:28:21.338912: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707802a00 of size 1792 next 36\n",
      "2024-03-22 12:28:21.338915: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707803100 of size 1792 next 35\n",
      "2024-03-22 12:28:21.338918: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707803800 of size 768 next 71\n",
      "2024-03-22 12:28:21.338921: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707803b00 of size 768 next 47\n",
      "2024-03-22 12:28:21.338924: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707803e00 of size 512 next 49\n",
      "2024-03-22 12:28:21.338928: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707804000 of size 512 next 51\n",
      "2024-03-22 12:28:21.338931: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707804200 of size 256 next 52\n",
      "2024-03-22 12:28:21.338934: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707804300 of size 256 next 60\n",
      "2024-03-22 12:28:21.338937: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707804400 of size 256 next 61\n",
      "2024-03-22 12:28:21.338940: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707804500 of size 1024 next 9\n",
      "2024-03-22 12:28:21.338943: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707804900 of size 2560 next 38\n",
      "2024-03-22 12:28:21.338946: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707805300 of size 256 next 55\n",
      "2024-03-22 12:28:21.338949: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707805400 of size 256 next 56\n",
      "2024-03-22 12:28:21.338952: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707805500 of size 256 next 59\n",
      "2024-03-22 12:28:21.338956: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 707805600 of size 4608 next 10\n",
      "2024-03-22 12:28:21.338959: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707806800 of size 7936 next 37\n",
      "2024-03-22 12:28:21.338962: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707808700 of size 65792 next 30\n",
      "2024-03-22 12:28:21.338965: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707818800 of size 7936 next 45\n",
      "2024-03-22 12:28:21.338969: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 70781a700 of size 1792 next 69\n",
      "2024-03-22 12:28:21.338972: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 70781ae00 of size 2560 next 57\n",
      "2024-03-22 12:28:21.338975: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 70781b800 of size 3584 next 46\n",
      "2024-03-22 12:28:21.338978: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 70781c600 of size 768 next 63\n",
      "2024-03-22 12:28:21.338981: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 70781c900 of size 256 next 64\n",
      "2024-03-22 12:28:21.338985: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 70781ca00 of size 256 next 65\n",
      "2024-03-22 12:28:21.338988: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 70781cb00 of size 3840 next 67\n",
      "2024-03-22 12:28:21.338991: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 70781da00 of size 256 next 68\n",
      "2024-03-22 12:28:21.338994: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 70781db00 of size 15616 next 29\n",
      "2024-03-22 12:28:21.338997: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707821800 of size 153088 next 25\n",
      "2024-03-22 12:28:21.339001: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707846e00 of size 131840 next 24\n",
      "2024-03-22 12:28:21.339004: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707867100 of size 820992 next 20\n",
      "2024-03-22 12:28:21.339008: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 70792f800 of size 608256 next 19\n",
      "2024-03-22 12:28:21.339013: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7079c4000 of size 2433024 next 15\n",
      "2024-03-22 12:28:21.339017: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707c16000 of size 2128896 next 48\n",
      "2024-03-22 12:28:21.339021: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707e1dc00 of size 608256 next 50\n",
      "2024-03-22 12:28:21.339025: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707eb2400 of size 131840 next 53\n",
      "2024-03-22 12:28:21.339029: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707ed2700 of size 36864 next 72\n",
      "2024-03-22 12:28:21.339033: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 707edb700 of size 36864 next 54\n",
      "2024-03-22 12:28:21.339036: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 707ee4700 of size 2128896 next 58\n",
      "2024-03-22 12:28:21.339039: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7080ec300 of size 2128896 next 70\n",
      "2024-03-22 12:28:21.339042: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7082f3f00 of size 11000064 next 66\n",
      "2024-03-22 12:28:21.339046: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 708d71800 of size 4292640000 next 62\n",
      "2024-03-22 12:28:21.339050: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 808b39500 of size 1488743168 next 18446744073709551615\n",
      "2024-03-22 12:28:21.339053: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: \n",
      "2024-03-22 12:28:21.339058: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 36 Chunks of size 256 totalling 9.0KiB\n",
      "2024-03-22 12:28:21.339062: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 512 totalling 1.0KiB\n",
      "2024-03-22 12:28:21.339066: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 5 Chunks of size 768 totalling 3.8KiB\n",
      "2024-03-22 12:28:21.339069: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 1024 totalling 2.0KiB\n",
      "2024-03-22 12:28:21.339073: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2024-03-22 12:28:21.339076: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 1792 totalling 5.2KiB\n",
      "2024-03-22 12:28:21.339079: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 2560 totalling 5.0KiB\n",
      "2024-03-22 12:28:21.339083: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 3584 totalling 3.5KiB\n",
      "2024-03-22 12:28:21.339087: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 7936 totalling 15.5KiB\n",
      "2024-03-22 12:28:21.339090: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 15616 totalling 15.2KiB\n",
      "2024-03-22 12:28:21.339094: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 36864 totalling 72.0KiB\n",
      "2024-03-22 12:28:21.339098: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 65792 totalling 64.2KiB\n",
      "2024-03-22 12:28:21.339102: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 131840 totalling 257.5KiB\n",
      "2024-03-22 12:28:21.339105: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 153088 totalling 149.5KiB\n",
      "2024-03-22 12:28:21.339109: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 608256 totalling 1.16MiB\n",
      "2024-03-22 12:28:21.339113: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 820992 totalling 801.8KiB\n",
      "2024-03-22 12:28:21.339116: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 2128896 totalling 4.06MiB\n",
      "2024-03-22 12:28:21.339120: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 2433024 totalling 2.32MiB\n",
      "2024-03-22 12:28:21.339124: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 11000064 totalling 10.49MiB\n",
      "2024-03-22 12:28:21.339127: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 4292640000 totalling 4.00GiB\n",
      "2024-03-22 12:28:21.339131: I external/local_tsl/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 4.02GiB\n",
      "2024-03-22 12:28:21.339136: I external/local_tsl/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 5803868160 memory_limit_: 5803868160 available bytes: 0 curr_region_allocation_bytes_: 11607736320\n",
      "2024-03-22 12:28:21.339145: I external/local_tsl/tsl/framework/bfc_allocator.cc:1114] Stats: \n",
      "Limit:                      5803868160\n",
      "InUse:                      4312987648\n",
      "MaxInUse:                   4373804544\n",
      "NumAllocs:                         204\n",
      "MaxAllocSize:               4292640000\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-03-22 12:28:21.339153: W external/local_tsl/tsl/framework/bfc_allocator.cc:497] ***************************************************************************_________________________\n",
      "2024-03-22 12:28:21.339179: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at bias_op.cc:274 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[10000,271,396] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer 'conv1d' (type Conv1D).\n\n{{function_node __wrapped__BiasAdd_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[10000,271,396] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:BiasAdd] name: \n\nCall arguments received by layer 'conv1d' (type Conv1D):\n  • inputs=tf.Tensor(shape=(10000, 275, 1), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m ys_test1 \u001b[38;5;241m=\u001b[39m to_categorical(ys_test)\n\u001b[1;32m     24\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mGradientExplainer(model, X_train)\n\u001b[0;32m---> 25\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXs_test1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m shap_values_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(shap_values[class_num])\n\u001b[1;32m     27\u001b[0m shap_values_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(shap_values_array)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/shap/explainers/_gradient.py:158\u001b[0m, in \u001b[0;36mGradientExplainer.shap_values\u001b[0;34m(self, X, nsamples, ranked_outputs, output_rank_order, rseed, return_variances)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshap_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, nsamples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, ranked_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output_rank_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, rseed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, return_variances\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the values for the model applied to X.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnsamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranked_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_rank_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_variances\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/shap/explainers/_gradient.py:268\u001b[0m, in \u001b[0;36m_TFGradient.shap_values\u001b[0;34m(self, X, nsamples, ranked_outputs, output_rank_order, rseed, return_variances)\u001b[0m\n\u001b[1;32m    266\u001b[0m     model_output_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_inputs, X)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 268\u001b[0m     model_output_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ranked_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_output:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_rank_order \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/shap/explainers/_gradient.py:395\u001b[0m, in \u001b[0;36m_TFGradient.run\u001b[0;34m(self, out, model_inputs, X)\u001b[0m\n\u001b[1;32m    393\u001b[0m     v \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(X[i]\u001b[38;5;241m.\u001b[39mreshape(shape), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_inputs[i]\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    394\u001b[0m     inputs\u001b[38;5;241m.\u001b[39mappend(v)\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mout\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer 'conv1d' (type Conv1D).\n\n{{function_node __wrapped__BiasAdd_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[10000,271,396] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:BiasAdd] name: \n\nCall arguments received by layer 'conv1d' (type Conv1D):\n  • inputs=tf.Tensor(shape=(10000, 275, 1), dtype=float32)"
     ]
    }
   ],
   "source": [
    "model_files = ['./models/cnnmodel.h5', './models/lstmmodel.h5']\n",
    "model_names = ['CNN', 'LSTM']\n",
    "\n",
    "for model_file, model_name in zip(model_files, model_names):\n",
    "        if model_name == 'LSTM':\n",
    "                model = load_model(model_file, custom_objects={'focal_loss_fixed': focal_loss})\n",
    "        else:\n",
    "                model = load_model(model_file)\n",
    "\n",
    "        feature_importance_df = pd.DataFrame()\n",
    "\n",
    "        for class_num in range(1, 7):\n",
    "            N_test = test_values[test_values[:,-2]==class_num]\n",
    "\n",
    "            if class_num == 1:\n",
    "                Xs_test = N_test[:10000,:-2]\n",
    "            else:\n",
    "                Xs_test = N_test[:,:-2]\n",
    "            ys_test = N_test[:len(N_test),-2]\n",
    "\n",
    "            Xs_test1 = Xs_test.reshape(Xs_test.shape[0], Xs_test.shape[1], 1)\n",
    "\n",
    "            ys_test1 = to_categorical(ys_test)\n",
    "            explainer = shap.GradientExplainer(model, X_train)\n",
    "            shap_values = explainer.shap_values(Xs_test1)\n",
    "            shap_values_array = np.array(shap_values[class_num])\n",
    "            shap_values_array = np.squeeze(shap_values_array)\n",
    "            shap_values_array.shape\n",
    "            agg_shap = shap_values_array.mean(0)\n",
    "            mean_weights = np.zeros((23, ))\n",
    "            for i in range(0, 275, 12):\n",
    "                mean_weights[i // 12] = np.mean(agg_shap[i:i+12])\n",
    "\n",
    "            # Sort based on weight:\n",
    "            indices_sort = np.argsort(-1 * mean_weights)\n",
    "            slices = np.arange(1, 24)\n",
    "\n",
    "            feature_importance_normalized = (mean_weights - mean_weights.min()) / (mean_weights.max() - mean_weights.min())\n",
    "\n",
    "            # Visualize:\n",
    "            fig, ax = plt.subplots(1, 2, figsize = (15, 4))\n",
    "            ax[0].bar(range(23), feature_importance_normalized[indices_sort])\n",
    "            ax[0].set_title('SHAP weights for class ' + str(class_num) + ' with a '+str(model_name)+' model')\n",
    "            ax[0].set_xticks(np.arange(23))\n",
    "            ax[0].set_xticklabels(slices[indices_sort])\n",
    "            ax[0].set_ylabel('SHAP weight')\n",
    "            ax[0].set_xlabel('Segment')\n",
    "\n",
    "            ecg_normalized = (Xs_test[20, :] - Xs_test[20, :].min()) / (Xs_test[20, :].max() - Xs_test[20, :].min())\n",
    "\n",
    "            ax[1].plot(np.arange(275), ecg_normalized , label = ' ECG for class ' + str(class_num))\n",
    "            ax[1].plot(np.repeat(feature_importance_normalized, 12), label = 'SHAP weight')\n",
    "            ax[1].set_title('SHAP weights for class ' + str(class_num) + ' with a '+str(model_name)+' model')\n",
    "            ax[1].set_ylabel('ECG signal / SHAP weights')\n",
    "            ax[1].set_xlabel('Time')\n",
    "            ax[1].legend()\n",
    "            plt.savefig('./output_pics/SHAP weights for class ' + str(class_num) + ' with a '+str(model_name)+' model', dpi = 400)\n",
    "\n",
    "            # Append the importance_dict to the feature_importance_df DataFrame\n",
    "            feature_importance_df['class_' + str(class_num)] = feature_importance_normalized\n",
    "        # Save the feature_importance_df DataFrame to a CSV file\n",
    "        feature_importance_df.to_csv('./output_csv/shap_'+ str(model_name) + '.csv', index=False)\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
